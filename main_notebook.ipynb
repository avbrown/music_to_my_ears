{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music to My Ears\n",
    "\n",
    "**Author:** [Vu Brown](https://www.linkedin.com/in/austin-brown-b5211384/)\n",
    "\n",
    "## Overview\n",
    "***\n",
    "This project develops a content-based filtering recommendation system for musical tracks by utilizing a multilabel binarizer as a preprocessing tool on a million user playlists. This creates a large scale utility matrix (1,000,000 by 2,262,292) comprised of playlists and tracks, which is used with Non-negative Matrix Factorization (NMF) along side cosine similarity to build the model.\n",
    "\n",
    "## Business Objective\n",
    "***\n",
    "This project explores unsupervised learning in an attempt to develop a musical track recommendation system simply based off of user playlists. Inspiration for this project came from Spotify's weekly generated playlist called Discover Weekly. The playlist consists of 30 tracks that the user has never heard before but are curtailed to the user's personal music preferences. Although I do not expect to create a recommendation system better than Spotify's Discover Weekly, I would like to explore a unique way this recommendation system could be built.\n",
    "\n",
    "## The Data\n",
    "***\n",
    "The data used in this project was sourced from:\n",
    "* A data package provided from [AIcrowd](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge/dataset_files) consisting of 1,000,000 playlists.\n",
    "* API calls to the [Spotify Web API|Spotify for Developers](https://developer.spotify.com/dashboard/applications).\n",
    "\n",
    "## Preprocessing\n",
    "***\n",
    "The goal of this section is to create a utility matrix consisting of playlists (rows) and tracks included in those playlists (columns). Since I'm unable to obtain actual Spotify user data, the assumption I'm making is that each playlist acts as an individual user's musical preference, so, if I were too build a utility matrix as described, I would be able to develop a content-based filtering recommendation system from users' musical preferences.\n",
    "\n",
    "The data package provided from AIcrowd was split into 1,000 separate JSON files which each included 1,000 playlists, totaling 1,000,000 playlists. The package also included a useful TEXT file, `stats.txt`, that had a basic summary of aspects regarding the dataset. The TEXT file was particularly useful in that it informed me to expect 2,262,292 unique tracks. Given this information, I expect the dimensions of the final utilitly matrix to be 1,000,000, by 2,262,292."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import entire modules\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    " \n",
    "# Import specific functions from modules\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTICE**\n",
    "\n",
    "* The data package from AIcrowd is much too large to upload to GitHub. You will have to navigate to the link above, download the data package (ZIP file - 5.39GB) to the project folder on your local computer, and then extract the contents from the ZIP file there.\n",
    "* This section of the notebook may require more than 8GB of RAM to run successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Matrix Set-Up\n",
    "\n",
    "The first objective is to create a `for` loop that iterates through each JSON file to ultimately create a DataFrame, `final_df`, with 1 million rows representing playlists and one column that consists of lists of tracks pertaining to each playlist.\n",
    "\n",
    "The first step to building the `for` loop is to read from the JSON files. Each of the JSON file names has two identifying features: an initial playlist number ending in 0, `initial_num`, and a final playlist number ending in 999, `final_num`. Applying an `incrementer` of 1,000 to both the initial and final playlist numbers within the `for` loop allows us to effectively read from each JSON file.\n",
    "\n",
    "Next, we have to pull the tracks from each playlist. This requires a nested `for` loop that populates a temporary list, `data`, with 1,000 lists where each list consists of each track in the playlist and each track has identifying information pertaining to it, shown below:\n",
    "   * `track_name` - the name of the track\n",
    "   * `track_uri` - the Spotify URI of the track\n",
    "   * `album_name` - the name of the track's album\n",
    "   * `album_uri` - the Spotify URI of the album\n",
    "   * `artist_name` - the name of the track's primary artist\n",
    "   * `artist_uri` - the Spotify URI of track's primary artist\n",
    "   * `duration_ms` - the duration of the track in milliseconds\n",
    "   * `pos` - the position of the track in the playlist (zero-based)\n",
    "\n",
    "Once `data` has been fully populated with 1,000 lists from the nested `for` loop, I convert `data` to a temporary DataFrame, `df`, with dimensions 1,000 by 1. I then manipulate the single column in `df` to create a new column that represents a list of tracks with only one identifying feature for a track opposed to all of the identifying features mentioned above. I also decided to use the `track_uri` instead of the `track_name` as the primary identifying feature for a track, so I could pull additional track data from Spotify's API later if needed.\n",
    "\n",
    "Since memory consumption is an issue with this dataset, using a single identifying feature for a track minimized this problem tremendously. As you will find, I also took additional measures throughout this section of the notebook to reduce memory consumption as best as I could.\n",
    "\n",
    "The final step to the `for` loop before iterating to the next JSON file is to concatenate `df` with `final_df`, essentially adding the list of tracks for each playlist from the currently open JSON file to the final DataFrame.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE VALUES!!!\n",
    "initial_num = 0\n",
    "final_num = 999\n",
    "incrementer = 1000\n",
    "\n",
    "# If memory is a limitation, reduce the `num_files` as needed.\n",
    "num_files = 1000\n",
    "\n",
    "# Declaring empty DataFrame, `final_d`\n",
    "# This DataFrame will consist of the full amount of playlists (aka 1,000,000)\n",
    "# and one column that consists of lists of tracks pertaining to each playlist.\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# The following `for` loop is used to iterate through each JSON file and populate `final_df`\n",
    "for file_index in range(0, num_files):\n",
    "    # `print` function shows the progress of the `for` loop\n",
    "    print(file_index)\n",
    "    \n",
    "    # Declaring empty list, `data`\n",
    "    data = []\n",
    "    \n",
    "    # Opening the JSON file\n",
    "    f = open(f'./spotify_million_playlist_dataset/data/mpd.slice.{initial_num}-{final_num}.json')\n",
    "    \n",
    "    # Creating a dictionary, `d` from the JSON data contained in `f`\n",
    "    d = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The following `for` loop is used to populate `data` with 1,000 lists.\n",
    "    # Each list pertains to each playlist and consists of each track in the playlist.\n",
    "    # Additionally, each track has identifying information pertaining to it.\n",
    "    for playlist in range(len(d['playlists'])):\n",
    "        tracks_list = d['playlists'][playlist]['tracks']\n",
    "        data.append(tracks_list)\n",
    "    \n",
    "    \n",
    "    # Converting `data` from a list to a DataFrame with dimensions 1,000 by 1\n",
    "    df = pd.DataFrame(pd.Series(data))\n",
    "    df.rename(columns={0: \"tracks\"}, inplace=True)\n",
    "    \n",
    "    # Creating an additional column within `df` that will inlcude lists of tracks with one identifying feature.\n",
    "    # The primary identifying feature chosen: `track_uri`\n",
    "    df['track_uris'] = df['tracks'].map(lambda x: [track['track_uri'] for track in x])\n",
    "    \n",
    "    # Dropping first column that is no longer needed and consequently reduces memory consumption\n",
    "    df.drop(columns='tracks', inplace=True)\n",
    "    \n",
    "    # Concatenating `final_df` with `df` \n",
    "    final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # Incrementing initial and final playlist numbers in order to select next JSON file\n",
    "    initial_num += incrementer\n",
    "    final_num += incrementer\n",
    "    \n",
    "    # Closing the currently open JSON file\n",
    "    f.close()\n",
    "\n",
    "# Reducing memory used by the following variables\n",
    "d = {}\n",
    "data = []\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: (1000000, 1)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices to `final_df`\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a good visual representation of the DataFrame in its current state\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel Binarizer\n",
    "\n",
    "`final_df` looks perfect so far! Just one more step to create the final utility matrix we desire. This will require the use of a multilabel binarizer to create a Compressed Sparse Row (CSR) matrix that will act as our final utility matrix.\n",
    "\n",
    "A multilabel binarizer will work wonders for what we want to accomplish. For one, we want to create tons of columns that represent each track in the entire dataset, and most importantly identify with a 1 (yes) or a 0 (no) if a specific track was included or not in any of the one million playlists. A multilable binarizer accomplishes just this, and, additionally, scikit-learn's `MultiLabelBinarizer` can output a CSR matrix if the `sparse_output` parameter is set to `True`. Since most of the elements in our utility matrix will be zero-valued, a CSR matrix will be an ideal output datatype for reducing memory consumption.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "U = mlb.fit_transform(final_df.pop('track_uris'))\n",
    "\n",
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dimensions of `U` are exactly what we had hoped for: 1,000,000 by 2,262,292!\n",
    "\n",
    "Additionally, look how little memory the CSR matrix uses! 48 bytes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: 48 (bytes)\n",
    "sys.getsizeof(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bypass having to run the lengthy/time-consuming block of code above, I'm going to save the CSR matrix as a NPZ file and the list of tracks and playlists as NPY files to a folder called `tmp`, so I can simply load them into the modeling notebook later. This will save lots of time down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('./tmp/U.npz', U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = mlb.classes_\n",
    "np.save('./tmp/tracks.npy', tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = np.asarray(final_df.index)\n",
    "np.save('./tmp/playlists.npy', playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have a utility matrix that looks promising, let's still convert the CSR matrix to a DataFrame in an effort to visually verify that the CSR matrix is a correct representation of the data.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame.sparse.from_spmatrix(U, index=playlists, columns=tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: (1000000, 2262292)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a good visual representation of the utility matrix in it's final state\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: 6 occurrences of this specific song\n",
    "final_df['spotify:track:0002yNGLtYSYtc0X6ZnFvp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the DataFrame version of the utility matrix appears to be a correct representation of the dataset, which infers that the CSR matrix is as well. It's now time to move on to the modeling section!\n",
    "\n",
    "Side Note: Look how significantly larger the DataFrame version of our utility matrix is in comparison to the CSR matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTICE: This block of code will take a while to run.\n",
    "# # Output Expectation: 531718224 (bytes)\n",
    "# sys.getsizeof(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "***\n",
    "The goal of this section is to create a musical track recommendation system by using the utility matrix created from the Preprocessing section of this notebook. Non-negative Matrix Factorization (NMF) and cosine similarity will be employed by the system to generate the track recommendations.\n",
    "\n",
    "### Spotify Web API\n",
    "In order for all code in this notebook to execute properly, you will need your own unique Client ID and Client Secret. Here is how to create a Spotify Web App through the Spotify Web API to obtain them:\n",
    "   * Create Spotify profile or sign in with your Spotify credentials [here](https://developer.spotify.com/dashboard/applications).\n",
    "   * On the Spotify for Developers Dashboard, navigate to the \"Create An App\" button and fill-in/agree to all items.\n",
    "   * Your unique Client ID and Client Secret will then be displayed within your newly created Web App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import entire modules\n",
    "import matplotlib.pyplot as plt\n",
    "import spotipy\n",
    " \n",
    "# Import specific functions from modules\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "####################################################################################\n",
    "######### Provide your unique Client ID and Client Secret as strings below #########\n",
    "####################################################################################\n",
    "SPOTIPY_CLIENT_ID = ''\n",
    "SPOTIPY_CLIENT_SECRET = ''\n",
    "####################################################################################\n",
    "\n",
    "# Establish Client Credentials Flow\n",
    "auth_manager = SpotifyClientCredentials(SPOTIPY_CLIENT_ID, SPOTIPY_CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data\n",
    "\n",
    "Load in the utility matrix, list of track URIs, and list of playlists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: 1000000x2262292 sparse matrix\n",
    "U = load_npz('./tmp/U.npz')\n",
    "\n",
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only 65,464,776 elements are stored in the utility matrix. According to `stats.txt`, there should be a total of 66,346,428 elements. It appears that ~1.3% of the tracks (aka 881,652 tracks) was lost when using the multilabel binarizer. Although, not majorly significant, I found it interesting enough to point out and perhaps resolve at a later time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation for length of `tracks`: 2262292\n",
    "tracks = np.load('./tmp/tracks.npy', allow_pickle=True)\n",
    "\n",
    "display(len(tracks))\n",
    "display(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation for length of `playlists`: 1000000\n",
    "playlists = np.load('./tmp/playlists.npy', allow_pickle=True)\n",
    "\n",
    "display(len(playlists))\n",
    "display(playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "Since there are so many tracks and playlists in the utility matrix, a dimensionality reduction algorithm needs to be employed, which is where NMF comes in. NMF factorizes one matrix with non-negative elements, in our case the utility matrix, into two separate matrices, W and H, which will resultingly also have non-negative elements. The non-negativity of the elements allows the resulting factorized matrices to be more easily interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $$U = WH$$ <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One dimension of the each of the factorized matrices will be significantly smaller, and in that dimension is where interesting, hidden features can be identified. The exact size of this dimension can also be tuned, and, as such, this is where the majority of my experimentation stemmed from. I used various hidden feature amounts (aka `n_components`) ranging from 2 to 10, and I began to notice that the algorithm was trying to match a major genre with each hidden feature. However, when the hidden feature amount was too small or too large, the algorithm wasn't optimized. This was easily identifiable when looking at the actual genres associated with the top tracks in each hidden feature and observing genre overlap amongst the top tracks. With that being said, the algorithm seemed to be categorizing tracks by genre at its best with 4 hidden features, which is why I have set `n_components` to 4 below.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 4\n",
    "\n",
    "model = NMF(n_components, verbose=10, random_state=1)\n",
    "H = model.fit_transform(U)\n",
    "W = model.components_\n",
    "\n",
    "np.save(f'./tmp/W_{n_components}.npy', W)\n",
    "np.save(f'./tmp/H_{n_components}.npy', H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load(f'./tmp/W_{n_components}.npy', allow_pickle=True)\n",
    "H = np.load(f'./tmp/H_{n_components}.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation for `W.shape`: (4, 2262292)\n",
    "# Output Expectation for `H.shape`: (1000000, 4)\n",
    "display(W.shape)\n",
    "display(H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility matrix has now be factorized into two matrices, `W` and `H`. `W` represents hidden features/genres vs. tracks, whereas `H` represents playlists vs. hidden features/genres.\n",
    "\n",
    "The following function, `interpret_track`, inputs new lines within long track names and inbetween artist names in order to clean up the text displayed in the plotting function below, `plot_top_tracks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_track(track_name, track_artists):\n",
    "    \n",
    "    track_name_counter = 0\n",
    "    updated_track_name = ''\n",
    "    \n",
    "    track_name_list = track_name.split(' ')\n",
    "    for word in track_name_list:\n",
    "        track_name_counter += len(word)\n",
    "        if track_name_counter <= 20:\n",
    "            updated_track_name += word + \" \"\n",
    "        else:\n",
    "            track_name_counter = 0\n",
    "            updated_track_name += \"\\n\" + word + \" \"\n",
    "            \n",
    "    \n",
    "    interpretable_track = ''\n",
    "    \n",
    "    if len(track_artists) == 1:\n",
    "        interpretable_track = updated_track_name + \"\\nby \" + track_artists[0]\n",
    "    elif len(track_artists) == 2:\n",
    "        interpretable_track = updated_track_name + \"\\nby \" + track_artists[0] + \"\\nand \" + track_artists[1]\n",
    "    else:\n",
    "        for index in range(len(track_artists)):\n",
    "            if index == 0:\n",
    "                interpretable_track = updated_track_name + \"\\nby \" + track_artists[index] + \",\"\n",
    "            elif (index > 0) and (index < len(track_artists) - 1):\n",
    "                interpretable_track += \"\\n\" + track_artists[index] + \",\"\n",
    "            else:\n",
    "                interpretable_track += \"\\nand \" + track_artists[index]\n",
    "                \n",
    "    return interpretable_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `plot_top_tracks`, takes the factorized matrix, `W`, which consists of hidden features associated with tracks, and plots the top tracks in each hidden feature. This function was adapted from a Jupyter Notebook found within Praveen Gowtham's GitHub repository located [here](https://github.com/admveen/NMF_tutorial/blob/master/CVID19_Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_tracks(n_components, W, tracks, num_top_tracks, title):\n",
    "    \n",
    "    colors = ['seagreen', 'chocolate', 'darkblue', 'firebrick']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_components, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for index, values in enumerate(W):\n",
    "        top_tracks_index = values.argsort()[: -num_top_tracks - 1 : -1]\n",
    "        top_tracks = [tracks[i] for i in top_tracks_index]\n",
    "                \n",
    "        # Pull track names and artist names using Spotify Web API\n",
    "        interpretable_top_tracks = []\n",
    "        for top_track in top_tracks:\n",
    "            track = sp.track(top_track)\n",
    "            track_name = track['name']\n",
    "            track_artists = [track['artists'][index]['name'] for index in range(len(track['artists']))]\n",
    "            interpretable_top_tracks.append(interpret_track(track_name, track_artists))\n",
    "        weights = values[top_tracks_index]\n",
    "\n",
    "        ax = axes[index]\n",
    "        ax.barh(interpretable_top_tracks, weights, height=0.6, color=colors[index])\n",
    "        ax.set_title(f\"Genre {index + 1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "        for border_line in \"top right\".split():\n",
    "            ax.spines[border_line].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.savefig('./images/feature_importance.jpg', dpi=300, transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_tracks = 10\n",
    "\n",
    "plot_top_tracks(n_components, W, tracks, num_top_tracks, \"Genres in NMF Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Genre 1 is mainly associated with Hip-Hop/Rap, Genre 2 with Pop/EDM, Genre 3 with Rock, and Genre 4 with Country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Hip-Hop/Rap', 'Pop/EDM', 'Rock', 'Country']\n",
    "colors = ['seagreen', 'chocolate', 'darkblue', 'firebrick']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Playlist\n",
    "\n",
    "Now, let's pull a specfic playlist that we would like to recommend tracks for. I personally enjoy playlist 253,699, which consists of EDM music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_0 = 253699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `track_uris_from_playlist`, returns a list of track URIs and a list of track indices associated with the tracks in `playlist_0`. The parameters of the function are the playlist number, `playlist_0`, and the list of all track URIs, `tracks`. The track URIs are useful for pulling track names and artist names from the Spotify API, which is what the next function is designed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_uris_from_playlist(playlist_0, tracks):\n",
    "    \n",
    "    np_playlist_0 = U[playlist_0].toarray()\n",
    "    playlist_track_indices = np.where(np_playlist_0[0] == 1)[0]\n",
    "    tracks_in_playlist_0 = [tracks[i] for i in playlist_track_indices]\n",
    "    \n",
    "    return (tracks_in_playlist_0, playlist_track_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `interpretable_tracks_in_playlist`, returns a list of tracks from `playlist_0` with their names and artists by using the Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def interpretable_tracks_in_playlist(playlist_0, tracks):\n",
    "\n",
    "    interpretable_tracks_in_playlist_0 = []\n",
    "    \n",
    "    tracks_in_playlist_0 = track_uris_from_playlist(playlist_0, tracks)[0]\n",
    "    for track_uri in tracks_in_playlist_0:\n",
    "        track = sp.track(track_uri)\n",
    "        track_name = track['name']\n",
    "        track_artists = [track['artists'][index]['name'] for index in range(len(track['artists']))]\n",
    "        interpretable_tracks_in_playlist_0.append(interpret_track(track_name, track_artists))\n",
    "\n",
    "    return interpretable_tracks_in_playlist_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the tracks in our playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, track in enumerate(interpretable_tracks_in_playlist(playlist_0, tracks)):\n",
    "    print('Track', index+1, ':')\n",
    "    print(track)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They sure are EDM tracks.\n",
    "\n",
    "The following function, `playlist_verification`, verifies that all of the tracks in `playlist_0` are present. It does this by comparing the tracks in `playlist_0` to the tracks found in the original JSON source file. This is useful since a small portion of the tracks in the utility matrix are missing. This function assures you that the playlist you wish to generate recommendations for is accurate when compared to it's original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlist_verification(playlist_0, tracks):\n",
    "    \n",
    "    tracks_in_playlist_0 = track_uris_from_playlist(playlist_0, tracks)[0]\n",
    "    \n",
    "    f = open(f'./spotify_million_playlist_dataset/data/mpd.slice.{str(playlist_0)[:3]}000-{str(playlist_0)[:3]}999.json')\n",
    "    d = json.load(f)\n",
    "    tracks_in_playlist_file = [d['playlists'][int(str(playlist_0)[-3:])]['tracks'][i]['track_uri']\\\n",
    "                               for i in range(len(d['playlists'][int(str(playlist_0)[-3:])]['tracks']))]\n",
    "    \n",
    "    tracks_in_playlist_0.sort()\n",
    "    tracks_in_playlist_file.sort()\n",
    "    \n",
    "    return tracks_in_playlist_0 == tracks_in_playlist_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: True\n",
    "# If False, I recommend choosing a different playlist.\n",
    "playlist_verification(playlist_0, tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have verified our playlist and seen first hand that our playlist has EDM tracks, let's take a look at the hidden feature/genre distribution of the playlist to see if our model categorized these tracks in the correct genre. This will require accessing `H`, the facorized matrix that consists of hidden features associated with playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_0 = H[playlist_0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `plot_playlist_genre_distribution`, plots the genre distribution within the playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_playlist_genre_distribution(H_0, genres, colors):\n",
    "    \n",
    "    H_0_percent_distribution = [value/np.sum(H_0) for value in H_0]\n",
    "\n",
    "    playlist_genre_distribution = []\n",
    "    playlist_colors = []\n",
    "    \n",
    "    for index in np.array(H_0_percent_distribution).argsort():\n",
    "        playlist_genre_distribution.append(genres[index])\n",
    "        playlist_colors.append(colors[index])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.barh(y=playlist_genre_distribution, width=np.sort(H_0_percent_distribution)*100, height=0.6, color=playlist_colors)\n",
    "    ax.set_title(\"Genre Distribution (%) Within Playlist\", fontdict={\"fontsize\": 14})\n",
    "    ax.set_xticks([num for num in range(0, 110, 10)])\n",
    "    for border_line in \"top right\".split():\n",
    "        ax.spines[border_line].set_visible(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_playlist_genre_distribution(H_0, genres, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms playlist 253,699 does consist of mainly Pop/EDM tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Now that we have selected our playlist and understand the genre distribution within the playlist, it is time to perform pair-wise cosine similarity between our playlist and all tracks. In doing this, we can find tracks that are most similar to our playlist and ultimately recommend those.\n",
    "\n",
    "Cosine similarity defines the cosine of the angle between two vectors as the dot product between the two vectors divided by the product of the magnitude of the two vectors, like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $$ \\cos_{track}(\\theta) = \\frac{H_0 \\cdot W^T_{track}}{||H_0||||W^T_{track}||}$$ <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the equation that we need to transpose the factorized matrix `W`. This is necessary so that there is no dimension incompatibility when performing pair-wise cosine similarity between our playlist and each track. After transposing `W`, cosine similarity can then be executed, and the cosine valus can be sorted from highest to lowest (highest being 1 and lowest being 0 in our case). Additionally, the indices for the cosine values can be obtained, specifically for the high cosine values, and these indices correlate directly to the indices of `tracks`, which has all of the Spotify track URIs. From there, we can begin adding tracks to a recommendation list, as long as the track is not already in the playlist and the track has a popularity score above 5, according to Spotify's popularity metric from 0 to 100. I decided to include this popularity metric as a factor to consider before adding a track to the recommendation list because I was noticing that many tracks that were being recommended were subjectively not good. I began to notice that most of these poor sounding tracks had a Spotify popularity score between 0 and 5, which is why I chose 5 as a cutoff. What I have explained in this segment is precisely what the function, `track_recommendations`, performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_recommendations(num_recommendations, W, H_0, playlist_0, tracks):\n",
    "    \n",
    "    # Transpose `W`\n",
    "    W_T = np.transpose(W)\n",
    "    \n",
    "    # Perform cosine similarity between `H_0` and `W_T`\n",
    "    cos_sim = cosine_similarity([H_0], W_T)\n",
    "    \n",
    "    # Sort values in `cos_sim` from highest to lowest,\n",
    "    # and obtain indices which correlate to track indices\n",
    "    track_recommendations = cos_sim[0].argsort()[:: -1]\n",
    "    \n",
    "    # Declare empty list, `final_recommendations_indices`\n",
    "    final_recommendations_indices = []\n",
    "    \n",
    "    # Loop through `track_recommendations`\n",
    "    for track_recommendation in track_recommendations:\n",
    "        track_recommendation_in_playlist_0 = False\n",
    "        \n",
    "        # End `for` loop when length of `final_recommendations_indices`\n",
    "        # is equal to `num_recommendations`\n",
    "        if len(final_recommendations_indices) == num_recommendations:\n",
    "            break\n",
    "        else:\n",
    "            # Check to see if recommended track is already in `playlist_0`\n",
    "            for playlist_track_index in track_uris_from_playlist(playlist_0, tracks)[1]:\n",
    "                if playlist_track_index == track_recommendation:\n",
    "                    track_recommendation_in_playlist_0 = True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # If recommended track is not in `playlist_0`, make sure the popularity of\n",
    "            # the track is at least above a 5 in accordance with Spotify API's \n",
    "            # popularity metric before adding to `final_recommendations_indices`\n",
    "            if track_recommendation_in_playlist_0 == False:\n",
    "                if sp.track(tracks[track_recommendation])['popularity'] > 5:\n",
    "                    final_recommendations_indices.append(track_recommendation)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    # Pull track URIs for list of final recommended tracks\n",
    "    final_recommendation_uris = [tracks[i] for i in final_recommendations_indices]\n",
    "    \n",
    "    # Convert track URIs to interpretable track names with artist names\n",
    "    interpretable_final_recommendations = []\n",
    "    for final_recommendation_uri in final_recommendation_uris:\n",
    "        track = sp.track(final_recommendation_uri)\n",
    "        track_name = track['name']\n",
    "        track_artists = [track['artists'][index]['name'] for index in range(len(track['artists']))]\n",
    "        interpretable_final_recommendations.append(interpret_track(track_name, track_artists))\n",
    "\n",
    "    return final_recommendation_uris, interpretable_final_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_recommendations = 10\n",
    "\n",
    "recommendation_uris, recommendations = track_recommendations(num_recommendations, W, H_0, playlist_0, tracks)\n",
    "for index, track in enumerate(recommendations):\n",
    "    print('Track Recommendation', index+1, ':')\n",
    "    print(recommendation_uris[index])\n",
    "    print(track)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, of the top 10 recommended tracks, tracks 1, 3, 4, 5, and 9, were EDM related, and tracks 2, 5, 7, 8, and 10 were Spanish-speaking related. This isn't ideal, but I would say that of the 5 EDM tracks I subjectively like 3 of them (tracks 1, 3, and 4), and I've never heard of any of the artists before which is great for being exposed to new artists. Unfortunately, the Spotify API does not have a track language attribute, which would be ideal for identifying tracks that are not in the English language and excluding them from the recommendation list.\n",
    "\n",
    "I implore you to try out other playlists and see what the model recommends. Simply alter the `playlist_0` and `num_recommendations` variabes and run the two blocks of code to generate track recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playlist_0 = 869717\n",
    "\n",
    "H_0 = H[playlist_0]\n",
    "\n",
    "print(\"Playlist Verified:\", playlist_verification(playlist_0, tracks))\n",
    "\n",
    "plot_playlist_genre_distribution(H_0, genres, colors)\n",
    "\n",
    "# # Uncomment the code below if you wish to see the names of the tracks and their artists\n",
    "# for index, track in enumerate(interpretable_tracks_in_playlist(playlist_0, tracks)):\n",
    "#     print('Track', index+1, ':')\n",
    "#     print(track)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_recommendations = 10\n",
    "\n",
    "recommendation_uris, recommendations = track_recommendations(num_recommendations, W, H_0, playlist_0, tracks)\n",
    "for index, track in enumerate(recommendations):\n",
    "    print('Track Recommendation', index+1, ':')\n",
    "    print(recommendation_uris[index])\n",
    "    print(track)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***\n",
    "Although the model is not great considering the track recommendations are a little wonky, consider what data was used: playlists. Not a single characteristic of a track was actually utilized. No audio features, no track names, no artist names, no release info, etc. Just playlists. This was a limitation I knew coming into this project, but, considering that simple fact, this model works surprisingly well in that it can even recommend a few tracks remotely similar to those within a given playlist.\n",
    "\n",
    "## Next Steps\n",
    "***\n",
    "When time permits, I would like to introduce track characteristics into this model and see how that would affect the model's performance. Spotify's Web API has just this capability as well when utilizing the function `audio_features` on a track. In addition, it could be useful to explore supervised learning techniques and build a classification model using Spotify's track popularity metric to predict top hits. Perhaps the combination of the two models could yield fascinating results.\n",
    "\n",
    "## Appendix\n",
    "***\n",
    "The following function, `plot_top_playlists`, takes a transposed factorized matrix, H, which consists of hidden features associated with playlists, and plots the top playlists in each hidden feature. This function can be particularly useful in identifing playlists that strongly lean towards a specific hidden feature. This function was adapted from a Jupyter Notebook found within Praveen Gowtham's GitHub repository located [here](https://github.com/admveen/NMF_tutorial/blob/master/CVID19_Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_playlists(n_components, H_T, playlists, num_top_playlists, title):\n",
    "    \n",
    "    colors = ['seagreen', 'chocolate', 'darkblue', 'firebrick']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_components, figsize=(20, 10), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for index, values in enumerate(H_T):\n",
    "        top_playlists_index = values.argsort()[: -num_top_playlists - 1 : -1]\n",
    "        top_playlists = [str(playlists[i]) for i in top_playlists_index]\n",
    "        weights = values[top_playlists_index]\n",
    "        \n",
    "        ax = axes[index]\n",
    "        ax.barh(top_playlists, weights, height=0.6, color=colors[index])\n",
    "        ax.set_title(f\"Genre {index + 1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "        for i in \"top right\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "H_T = np.transpose(H)\n",
    "plot_top_playlists(n_components, H_T, playlists, 10, \"Genres in NMF Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
