{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Goal\n",
    "The goal of this notebook is to create a utility matrix consisting of playlists (rows) and tracks included in those playlists (columns). The data package provided from AIcrowd, [here](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge/dataset_files), was split into 1,000 separate JSON files which each included 1,000 playlists, totaling 1,000,000 playlists. The package also included a useful TEXT file, `stats.txt`, that had a basic summary of particular aspects of the dataset. The TEXT file was particularly useful in that it informed me to expect 2,262,292 unique tracks. Given this information, I expect the dimenstions of the final utilitly matrix to be 1,000,000, by 2,262,292.\n",
    "\n",
    "**NOTICE**\n",
    "\n",
    "* The data package from AIcrowd is much too large to upload too GitHub. You will have to navigate to the link above, download the data package (ZIP file - 5.39GB) to the project folder on your local computer, and then extract the contents from the ZIP file there.\n",
    "* This notebook may require more than 8GB of RAM to run successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import entire modules\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    " \n",
    "# Import specific functions from modules\n",
    "# from pathlib import Path\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# Remove warnings as required\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first objective is to create a `for` loop that iterates through each JSON file to ultimately create a DataFrame, `final_df`, with 1 million rows representing playlists and one column that consists of lists of tracks pertaining to each playlist.\n",
    "\n",
    "The first step to building the `for` loop is to read from the JSON files. Each of the JSON file names has two identifying features: an initial playlist number ending in 0, `initial_num`, and a final playlist number ending in 999, `final_num`. Applying an `incrementer` of 1,000 to both the initial and final playlist numbers within the `for` loop allows us to effectively read from each JSON file.\n",
    "\n",
    "Next, we have to pull the tracks from each playlist. This requires a nested `for` loop that populates a temporary list, `data`, with 1,000 lists where each list consists of each track in the playlist and each track has identifying information pertaining to it, shown below:\n",
    "   * `track_name` - the name of the track\n",
    "   * `track_uri` - the Spotify URI of the track\n",
    "   * `album_name` - the name of the track's album\n",
    "   * `album_uri` - the Spotify URI of the album\n",
    "   * `artist_name` - the name of the track's primary artist\n",
    "   * `artist_uri` - the Spotify URI of track's primary artist\n",
    "   * `duration_ms` - the duration of the track in milliseconds\n",
    "   * `pos` - the position of the track in the playlist (zero-based)\n",
    "\n",
    "Once `data` has been fully populated with 1,000 lists from the nested `for` loop, I convert `data` to a temporary DataFrame, `df`, with dimensions 1,000 by 1. I then manipulate the single column in `df` to create a new column that represents a list of tracks with only one identifying feature for a track opposed to all of the identifying features mentioned above. I also decided to use the `track_uri` instead of the `track_name` as the primary identifying feature for a track, so I could pull additional track data from Spotify's API later if needed.\n",
    "\n",
    "Since memory consumption is an issue with this dataset, using a single identifying feature for a track minimized this problem tremendously. As you will find, I also took additional measures throughout this notebook to reduce memory consumption as best as I could.\n",
    "\n",
    "The final step to the `for` loop before iterating to the next JSON file is to concatenate `df` with `final_df`, essentially adding the list of tracks for each playlist from the currently open JSON file to the final DataFrame.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE VALUES!!!\n",
    "initial_num = 0\n",
    "final_num = 999\n",
    "incrementer = 1000\n",
    "\n",
    "# If memory is a limitation, reduce the `num_files` as needed.\n",
    "num_files = 1000\n",
    "\n",
    "# Declaring empty DataFrame, `final_d`\n",
    "# This DataFrame will consist of the full amount of playlists (aka 1,000,000)\n",
    "# and one column that consists of lists of tracks pertaining to each playlist.\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# The following `for` loop is used to iterate through each JSON file and populate `final_df`\n",
    "for file_index in range(0, num_files):\n",
    "    # `print` function shows the progress of the `for` loop\n",
    "    print(file_index)\n",
    "    \n",
    "    # Declaring empty list, `data`\n",
    "    data = []\n",
    "    \n",
    "    # Opening the JSON file\n",
    "    f = open(f'./spotify_million_playlist_dataset/data/mpd.slice.{initial_num}-{final_num}.json')\n",
    "    \n",
    "    # Creating a dictionary, `d` from the JSON data contained in `f`\n",
    "    d = json.load(f)\n",
    "    \n",
    "    \n",
    "    # The following `for` loop is used to populate `data` with 1,000 lists.\n",
    "    # Each list pertains to each playlist and consists of each track in the playlist.\n",
    "    # Additionally, each track has identifying information pertaining to it.\n",
    "    for playlist in range(len(d['playlists'])):\n",
    "        tracks_list = d['playlists'][playlist]['tracks']\n",
    "        data.append(tracks_list)\n",
    "    \n",
    "    \n",
    "    # Converting `data` from a list to a DataFrame with dimensions 1,000 by 1\n",
    "    df = pd.DataFrame(pd.Series(data))\n",
    "    df.rename(columns={0: \"tracks\"}, inplace=True)\n",
    "    \n",
    "    # Creating an additional column within `df` that will inlcude lists of tracks with one identifying feature.\n",
    "    # The primary identifying feature chosen: `track_uri`\n",
    "    df['track_uris'] = df['tracks'].map(lambda x: [track['track_uri'] for track in x])\n",
    "    \n",
    "    # Dropping first column that is no longer needed and consequently reduces memory consumption\n",
    "    df.drop(columns='tracks', inplace=True)\n",
    "    \n",
    "    # Concatenating `final_df` with `df` \n",
    "    final_df = pd.concat([final_df, df])\n",
    "    \n",
    "    # Incrementing initial and final playlist numbers in order to select next JSON file\n",
    "    initial_num += incrementer\n",
    "    final_num += incrementer\n",
    "    \n",
    "    # Closing the currently open JSON file\n",
    "    f.close()\n",
    "\n",
    "# Reducing memory used by the following variables\n",
    "d = {}\n",
    "data = []\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: (1000000, 1)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices to `final_df`\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a good visual representation of the DataFrame in its current state\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`final_df` looks perfect so far! Just one more step to create the final utility matrix we desire. This will require the use of a multilabel binarizer to create a Compressed Sparse Row (CSR) matrix that will act as our final utility matrix.\n",
    "\n",
    "A multilabel binarizer will work wonders for what we want to accomplish. For one, we want to create tons of columns that represent each track in the entire dataset, and most importantly identify with a 1 (yes) or a 0 (no) if a specific track was included or not in any of the one million playlists. A multilable binarizer accomplishes just this, and, additionally, scikit-learn's `MultiLabelBinarizer` can output a CSR matrix if the `sparse_output` parameter is set to `True`. Since most of the elements in our utility matrix will be zero-valued, a CSR matrix will be an ideal output datatype for reducing memory consumption.\n",
    "\n",
    "**NOTICE**: This block of code will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "U = mlb.fit_transform(final_df.pop('track_uris'))\n",
    "\n",
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dimensions of `U` are exactly what we had hoped for: 1,000,000 by 2,262,292!\n",
    "\n",
    "Additionally, look how little memory the CSR matrix uses! 48 bytes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: 48 (bytes)\n",
    "sys.getsizeof(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bypass having to run the lengthy/time-consuming block of code above, I'm going to save the CSR matrix as a NPZ file and the list of tracks and playlists as NPY files to a folder called `tmp`, so I can simply load them into the modeling notebook later. This will save lots of time down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('./tmp/U.npz', U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = mlb.classes_\n",
    "np.save('./tmp/tracks.npy', tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = np.asarray(final_df.index)\n",
    "np.save('./tmp/playlists.npy', playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = load_npz('./tmp/U.npz')\n",
    "\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load('./tmp/tracks.npy', allow_pickle=True)\n",
    "\n",
    "display(len(tracks))\n",
    "display(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlists = np.load('./tmp/playlists.npy', allow_pickle=True)\n",
    "\n",
    "display(len(playlists))\n",
    "display(playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that loading in the NZP and NPY files we created earlier is working well!\n",
    "\n",
    "Although we have a utility matrix that looks promising, let's still convert the CSR matrix to a DataFrame in an effort to visually verify that the CSR matrix is a correct representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame.sparse.from_spmatrix(U, index=playlists, columns=tracks)\n",
    "\n",
    "# final_df = pd.DataFrame.sparse.from_spmatrix(U, index=final_df.index, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: (1000000, 2262292)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a good visual representation of the utility matrix in it's final state\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Expectation: 6 occurrences of this specific song\n",
    "final_df['spotify:track:0002yNGLtYSYtc0X6ZnFvp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the DataFrame version of the utility matrix appears to be a correct representation of the dataset, which infers that the CSR matrix is as well. It's now time to move on to the modeling phase!\n",
    "\n",
    "Side Note: Look how significantly larger the DataFrame version of our utility matrix is in comparison to the CSR matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTICE: This block of code will take a while to run.\n",
    "# # Output Expectation: 531718224 (bytes)\n",
    "# sys.getsizeof(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This will take obsolutely forever to run for this DataFrame,\n",
    "# # but this could be useful down the road.\n",
    "\n",
    "# filepath = Path('./output_files/playlists_vs_songs.csv')  \n",
    "# filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "# final_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
